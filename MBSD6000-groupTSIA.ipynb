{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-g5By3P4tavy"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, \n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4DN769E2O_R"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vt-StAAZguA"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install gdown --no-use-pep517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcBA19FlDPZO"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install -y unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSeyZMq-BYsu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FB7gLU4F54l"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdQrL_rwa-1K"
   },
   "outputs": [],
   "source": [
    "gdown.download(\n",
    "    url='https://drive.google.com/uc?id=1Ag0jd21oRwJhVFIBohmX_ogeojVtapLy',\n",
    "    output='bard.zip',\n",
    "    quiet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2YW4GGa9Y5o"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "unzip -qo bard.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js75OARBF_B8"
   },
   "source": [
    "# Export pretrained word vectors to TF-Hub module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DY5Ze6pO1G5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.vec.gz\n",
    "curl -O https://raw.githubusercontent.com/tensorflow/hub/master/examples/text_embeddings_v2/export_v2.py\n",
    "gunzip -qf cc.bn.300.vec.gz --k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tkv5acr_Q9UU"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python export_v2.py --embedding_file=cc.bn.300.vec --export_path=text_module --num_lines_to_ignore=1 --num_lines_to_use=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9WEpmedF_3_"
   },
   "outputs": [],
   "source": [
    "module_path = \"text_module\"\n",
    "embedding_layer = hub.KerasLayer(module_path, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1MBnaBUihWn"
   },
   "outputs": [],
   "source": [
    "embedding_layer(['বাস', 'বসবাস', 'ট্রেন', 'যাত্রী', 'ট্রাক']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KY8LiFOHmcd"
   },
   "source": [
    "# Convert to Tensorflow Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYv6LqlEChO1"
   },
   "outputs": [],
   "source": [
    "dir_names = ['economy', 'sports', 'entertainment', 'state', 'international']\n",
    "\n",
    "file_paths = []\n",
    "labels = []\n",
    "for i, dir in enumerate(dir_names):\n",
    "  file_names = [\"/\".join([dir, name]) for name in os.listdir(dir)]\n",
    "  file_paths += file_names\n",
    "  labels += [i] * len(os.listdir(dir))\n",
    "  \n",
    "np.random.seed(42)\n",
    "permutation = np.random.permutation(len(file_paths))\n",
    "\n",
    "file_paths = np.array(file_paths)[permutation]\n",
    "labels = np.array(labels)[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mimhWVSzzAmS"
   },
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "train_size = int(len(file_paths) * train_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BNXFrkotAYu"
   },
   "outputs": [],
   "source": [
    "# plot training vs validation distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(labels[0:train_size])\n",
    "plt.title(\"Train labels\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(labels[train_size:])\n",
    "plt.title(\"Validation labels\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZRGTzEhUi7Q"
   },
   "outputs": [],
   "source": [
    "def load_file(path, label):\n",
    "    return tf.io.read_file(path), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g4nRflB7fbF"
   },
   "outputs": [],
   "source": [
    "def make_datasets(train_size):\n",
    "  batch_size = 256\n",
    "\n",
    "  train_files = file_paths[:train_size]\n",
    "  train_labels = labels[:train_size]\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "  train_ds = train_ds.map(load_file).shuffle(5000)\n",
    "  train_ds = train_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  test_files = file_paths[train_size:]\n",
    "  test_labels = labels[train_size:]\n",
    "  test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
    "  test_ds = test_ds.map(load_file)\n",
    "  test_ds = test_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PuuN6el8tv9"
   },
   "outputs": [],
   "source": [
    "train_data, validation_data = make_datasets(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrdZI6FqPJNP"
   },
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhCqbDK2uUV5"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHUw807XPPM9"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=[], dtype=tf.string),\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5),\n",
    "  ])\n",
    "  model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=\"adam\", metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5J4EXJUmPVNG"
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# Create earlystopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ7XJLg2u2No"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoBkN2tAaXWD"
   },
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_data, \n",
    "                    validation_data=validation_data, \n",
    "                    epochs=5, \n",
    "                    callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoDk8otmMoT7"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6tOnByIOeGn"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dptEywzZJk4l"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dzeml6Pk0ub"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4M3Lzg8jHcB"
   },
   "outputs": [],
   "source": [
    "samples = file_paths[0:3]\n",
    "for i, sample in enumerate(samples):\n",
    "  f = open(sample)\n",
    "  text = f.read()\n",
    "  print(text[0:100])\n",
    "  print(\"True Class: \", sample.split(\"/\")[0])\n",
    "  print(\"Predicted Class: \", dir_names[y_pred[i]])\n",
    "  f.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlDTIpMBu6h-"
   },
   "source": [
    "## Compare Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqrERUCS1Xn7"
   },
   "outputs": [],
   "source": [
    "y_true = np.array(labels[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NX5w-NuTKuVP"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=dir_names))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IDdZSPcLtKx4"
   ],
   "name": "Bangla article classifier.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
